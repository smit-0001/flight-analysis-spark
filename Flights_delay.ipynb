{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flights Delay Prediction using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and session building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression , RandomForestRegressor , DecisionTreeRegressor, GBTRegressor\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator , RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/29 19:54:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|DayofMonth|DayOfWeek|Carrier|OriginAirportID|DestAirportID|DepDelay|ArrDelay|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "|        19|        5|     DL|          11433|        13303|      -3|       1|\n",
      "|        19|        5|     DL|          14869|        12478|       0|      -8|\n",
      "|        19|        5|     DL|          14057|        14869|      -4|     -15|\n",
      "+----------+---------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv = spark.read.csv('flights.csv', inferSchema=True, header=True)\n",
    "csv.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting departure delay into boolean label field to use in classification model\n",
    "specifically a flight that departed late by 30 mins is marked as 1 and filghts that are departed early or late by less than 30 mins are marked as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+---------------+-------------+--------+--------+\n",
      "|DayOfWeek|DayOfMonth|Carrier|OriginAirportID|DestAirportID|ArrDelay|DepDelay|\n",
      "+---------+----------+-------+---------------+-------------+--------+--------+\n",
      "|        5|        19|     DL|          11433|        13303|       1|      -3|\n",
      "|        5|        19|     DL|          14869|        12478|      -8|       0|\n",
      "|        5|        19|     DL|          14057|        14869|     -15|      -4|\n",
      "+---------+----------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = csv.select(\"DayOfWeek\", \"DayOfMonth\", \"Carrier\", \"OriginAirportID\", \"DestAirportID\", \"ArrDelay\", \"DepDelay\")\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use 70% of the data for training, and reserve 30% for testing. In the testing data, the label column is renamed to trueLabel so I can use it later to compare predicted labels with known actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Rows: 1891799 \n",
      " Testing Rows: 811823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "# test = splits[1].withColumnRenamed(\"label\", \"trueLabel\")\n",
    "# test = splits[1].withColumnRenamed(\"DepDelay\", \"trueLabel\")\n",
    "\n",
    "print(\"Training Rows:\", train.count(), \"\\n\", \"Testing Rows:\", test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+---------------+-------------+--------+--------+\n",
      "|DayOfWeek|DayOfMonth|Carrier|OriginAirportID|DestAirportID|ArrDelay|DepDelay|\n",
      "+---------+----------+-------+---------------+-------------+--------+--------+\n",
      "|        1|         1|     9E|          10397|        12191|     -18|      -2|\n",
      "|        1|         1|     9E|          10397|        12264|     -25|      -3|\n",
      "|        1|         1|     9E|          10397|        12264|       6|      -5|\n",
      "+---------+----------+-------+---------------+-------------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+---------------+-------------+--------+---------+\n",
      "|DayOfWeek|DayOfMonth|Carrier|OriginAirportID|DestAirportID|ArrDelay|trueLabel|\n",
      "+---------+----------+-------+---------------+-------------+--------+---------+\n",
      "|        1|         1|     9E|          10397|        12191|     -18|       -3|\n",
      "|        1|         1|     9E|          10397|        13851|      -7|       -2|\n",
      "|        1|         1|     9E|          10397|        13851|       2|        2|\n",
      "+---------+----------+-------+---------------+-------------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipepine and Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline consists of a series of transformer and estimator stages that typically prepare a DataFrame for modeling and then train a predictive model. In this case, you will create a pipeline with seven stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A StringIndexer estimator that converts string values to indexes for categorical features\n",
    "\n",
    "* A VectorAssembler that combines categorical features into a single vector\n",
    "\n",
    "* A VectorIndexer that creates indexes for a vector of categorical features\n",
    "\n",
    "* A VectorAssembler that creates a vector of continuous numeric features\n",
    "\n",
    "* A MinMaxScaler that normalizes continuous numeric features\n",
    "\n",
    "* A VectorAssembler that creates a vector of categorical and continuous features\n",
    "\n",
    "* A 3 Classifiers that trains a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "strIdx = StringIndexer(inputCol = \"Carrier\", \n",
    "                        outputCol = \"CarrierIdx\")\n",
    "\n",
    "catVect = VectorAssembler(inputCols = [\"CarrierIdx\", \"DayOfMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\",\"ArrDelay\"], \n",
    "                          outputCol=\"catFeatures\")\n",
    "\n",
    "catIdx = VectorIndexer( inputCol = catVect.getOutputCol(), \n",
    "                        outputCol = \"idxCatFeatures\")\n",
    "\n",
    "numVect = VectorAssembler(inputCols = [\"ArrDelay\"], \n",
    "                          outputCol=\"numFeatures\")\n",
    "\n",
    "minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), \n",
    "                      outputCol=\"normFeatures\")\n",
    "\n",
    "featVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"normFeatures\"], \n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 4 Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"catFeatures\", labelCol=\"DepDelay\")\n",
    "rf = RandomForestRegressor(featuresCol=\"catFeatures\", labelCol=\"DepDelay\" , numTrees=15)\n",
    "dt = DecisionTreeRegressor(featuresCol=\"catFeatures\", labelCol=\"DepDelay\" , maxDepth=5)\n",
    "gbt = GBTRegressor(featuresCol=\"catFeatures\", labelCol=\"DepDelay\" , maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, lr])\n",
    "pipeline_rf = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, rf])\n",
    "pipeline_dt = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, dt])\n",
    "pipeline_gbt = Pipeline(stages=[strIdx, catVect, catIdx, numVect, minMax, featVect, gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Model on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/29 20:33:09 WARN Instrumentation: [87ce971a] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plModel_lr = pipeline_lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 127:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/29 20:34:04 WARN MemoryStore: Not enough space to cache rdd_507_1 in memory! (computed 67.3 MiB so far)\n",
      "23/09/29 20:34:04 WARN BlockManager: Persisting block rdd_507_1 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plModel_rd = pipeline_rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plModel_dt = pipeline_dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plModel_gbt = pipeline_gbt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making prediction on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lr = plModel_lr.transform(test)\n",
    "prediction_rf = plModel_rd.transform(test)\n",
    "prediction_dt = plModel_dt.transform(test)\n",
    "prediction_gbt = plModel_gbt.transform(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an instance of RegressionEvaluator\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"mae\"  )\n",
    "evaluator_mse = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"mse\"  )\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"r2\"  )\n",
    "\n",
    "# Calculate the evaluation metric for Linear regression model\n",
    "mae_lr = evaluator_mae.evaluate(prediction_lr)\n",
    "mse_lr = evaluator_mse.evaluate(prediction_lr)\n",
    "r2_lr = evaluator_r2.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mae_rf = evaluator_mae.evaluate(prediction_rf)\n",
    "mse_rf = evaluator_mse.evaluate(prediction_rf)\n",
    "r2_rf = evaluator_r2.evaluate(prediction_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mae_dt = evaluator_mae.evaluate(prediction_dt)\n",
    "mse_dt = evaluator_mse.evaluate(prediction_dt)\n",
    "r2_dt = evaluator_r2.evaluate(prediction_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mae_gbt = evaluator_mae.evaluate(prediction_gbt)\n",
    "mse_gbt = evaluator_mse.evaluate(prediction_gbt)\n",
    "r2_gbt = evaluator_r2.evaluate(prediction_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 281:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+------------------+------------------+\n",
      "|          Model Name|              MAE|               MSE|                R2|\n",
      "+--------------------+-----------------+------------------+------------------+\n",
      "|   Linear Regression|8.364091696765529|150.55199715628916|0.8831821518290619|\n",
      "|       Random Forest|9.837720626903483| 421.2451051510495|0.6731431819851453|\n",
      "|Decision Tree Reg...|7.639199489024753| 327.1529296471188|0.7461521468591279|\n",
      "|Gradient Boost Re...|7.473643727135636| 320.1897720435061|0.7515550714505879|\n",
      "+--------------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = spark.createDataFrame([ (\"Linear Regression\",mae_lr, mse_lr,r2_lr ) ,\n",
    "                                  (\"Random Forest\",mae_rf, mse_rf,r2_rf ) ,\n",
    "                                  (\"Decision Tree Regression\",mae_dt, mse_dt,r2_dt ) ,\n",
    "                                  (\"Gradient Boost Regressor\",mae_gbt, mse_gbt,r2_gbt) \n",
    "                                ],\n",
    "\n",
    "                                [\"Model Name\",\"MAE\", \"MSE\",\"R2\"]\n",
    "                              )\n",
    "metrics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing prediction of each model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/09/29 21:05:51 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1053)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
      "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:293)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "# lr_output = prediction_lr.select(\"trueLabel\", \"prediction\").withColumnRenamed(\"prediction\", \"lr_prediction\").limit(10)\n",
    "# dt_output = prediction_rf.select(\"trueLabel\", \"prediction\").withColumnRenamed(\"prediction\", \"dt_prediction\").limit(10)\n",
    "# rf_output = prediction_dt.select(\"trueLabel\", \"prediction\").withColumnRenamed(\"prediction\", \"rf_prediction\").limit(10)\n",
    "# gbt_output = prediction_gbt.select(\"trueLabel\", \"prediction\").withColumnRenamed(\"prediction\", \"gbt_prediction\").limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Join the predictions from all four models based on label\n",
    "# combined_predictions = lr_output \\\n",
    "#     .join(dt_output, on=\"trueLabel\", how=\"inner\") \\\n",
    "#     .join(rf_output, on=\"trueLabel\", how=\"inner\") \\\n",
    "#     .join(gbt_output, on=\"trueLabel\", how=\"inner\")\n",
    "\n",
    "# # Show the combined predictions\n",
    "# combined_predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
